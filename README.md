# MySQL to S3 ETL Pipeline with Jenkins

This project implements an automated ETL pipeline that extracts employee data from a MySQL database, transforms it by adding a calculated bonus column, and loads the cleaned dataset into an AWS S3 bucket. The pipeline uses Python (pandas, mysql-connector-python, boto3, python-dotenv) to perform the data operations, with environment variables stored securely in a `.env` file locally and in Jenkins credentials for CI/CD runs. The Jenkins pipeline, defined in the `Jenkinsfile`, consists of stages to check out the latest code from GitHub, set up a Python virtual environment, install dependencies, run the ETL script, and archive logs. The final CSV is uploaded to S3 in a date-partitioned format (`raw/YYYY/MM/DD/employee_data.csv`) . The job is triggered automatically via a GitHub webhook whenever code is pushed to the repository, ensuring the data pipeline stays up to date without manual intervention. This setup demonstrates end-to-end automation of data ingestion from a relational database to cloud storage with proper use of credentials, environment separation, and CI/CD best practices.
